# Reinforcement Learning for Next-Generation AML - From Rigid Rules to Dynamic, Transparent Decisioning
A Practical Guide Combining Causal RL, and Thompson-Sampling for Watch-List Prioritization and Peer-Group Outlier Detection
Anti-Money Laundering (AML) operations are facing a dual challenge: growing regulatory pressure and increasing transaction complexity. Traditional rule-based systems - such as fixed watch-list filters and static thresholds - are becoming costly and ineffective, generating high false-positive rates and missing sophisticated patterns like smurfing chains or dormant-to-active account spikes. Meanwhile, banks are under pressure to balance compliance with operational efficiency, ensuring every alert and every Suspicious Activity Report (SAR) brings measurable value.
This paper introduces a reinforcement learning (RL)–driven framework for AML, Causal RL, and Thompson-Sampling bandits with unsupervised methods like HDBSCAN clustering. Instead of relying on reactive rules, our system learns from historical sequences and real-time signals, producing risk-aware, explainable policies. The proposed pipeline is not just a proof of concept; early pilots in transaction monitoring have already shown up to 18% fewer false CTRs and 8% higher SAR yield, with clear audit trails that regulators trust.
Abstract
We introduce an RL framework to transform bank AML from static "report + rule" setups into real‑time, self‑optimizing policies.
Two core modules:
Causal RL with Decision Transformers for Dormant‑to‑Active spikes - uses structural causal models and counterfactual policy gradients to link interventions (freeze, limit) to fraud loss and customer impact.
Thompson‑Sampling Bandit + HDBSCAN or triaging watch-list and peer-group outlier alerts. This approach clusters behavioral profiles to estimate risk levels and dynamically allocates analyst attention to maximize the efficiency of suspicious activity report (SAR) reviews.

We outline the required data inputs - transaction history, KYC profiles, and watch-list flags - and describe an offline-replay method to safely train decision models. The framework supports real-time alert handling and gradually refines its policy through continuous learning. By combining causal inference, behavior clustering, and adaptive RL, the system can outperform static rules, improving detection precision, operational efficiency, and regulatory transparency.
